<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.40.1" />
  <meta name="author" content="Yin Cui">

  
  
  
  
  <meta name="description" content="Research Scientist">

  
  <link rel="alternate" hreflang="en-us" href="/">

  


  

  
  
  <meta name="theme-color" content="#0095eb">
  
  
  
  
    
  
  
    
    
      
        <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
      
    
  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.1/css/academicons.min.css" integrity="sha512-NThgw3XKQ1absAahW6to7Ey42uycrVvfNfyjqcFNgCmOCQ5AR4AO0SiXrN+8ZtYeappp56lk1WtvjVmEa+VR6A==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">
  
  
  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700%7cRoboto:400,400italic,700%7cRoboto&#43;Mono">
  
  <link rel="stylesheet" href="/styles.css">
  
  <link rel="stylesheet" href="/css/yincui.css">
  

  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-54472398-1', 'auto');
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="//www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  

  
  <link rel="alternate" href="/index.xml" type="application/rss+xml" title="Yin Cui">
  <link rel="feed" href="/index.xml" type="application/rss+xml" title="Yin Cui">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@https://twitter.com/YinCui1">
  <meta property="twitter:creator" content="@https://twitter.com/YinCui1">
  
  <meta property="og:site_name" content="Yin Cui">
  <meta property="og:url" content="/">
  <meta property="og:title" content="Yin Cui">
  <meta property="og:description" content="Research Scientist">
  <meta property="og:locale" content="en-us">
  
  <meta property="og:updated_time" content="2023-03-07T00:00:00&#43;00:00">
  

  

  <title>Yin Cui</title>

</head>
<body id="top" data-spy="scroll" data-target="#navbar-main" data-offset="71" >

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
      <a class="navbar-brand" href="/">Yin Cui</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      
      <ul class="nav navbar-nav navbar-right">
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#about" data-target="#about">
            
            <span>About</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#lists" data-target="#lists">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#misc" data-target="#misc">
            
            <span>Misc</span>
            
          </a>
        </li>

        
        
      

      
      </ul>

    </div>
  </div>
</nav>



<span id="homepage" style="display: none"></span>


  





  
  
  
  <section id="about" class="home-section">
    <div class="container">
      



<div class="row">
  <div class="col-xs-12 col-md-4">
    <div id="profile">

      
      <div class="portrait" itemprop="image"
           style="background-image: url('/img/portrait.jpg');">
      </div>
      

      <div class="portrait-title">
        <h2 itemprop="name">Yin Cui</h2>
        <h3 itemprop="jobTitle">Research Scientist at Nvidia</h3>  
      </div>
    </div>
  </div>

  <div class="col-xs-12 col-md-8" itemprop="description">
    <h1>About</h1>
    
    <p>Hi, I am Yin Cui (崔崟 in Chinese, pronounced as /yin tsui/), a research scientist at NVIDIA. Before joining Nvidia, I was a research scientist at Google. I obtained my Ph.D. in Computer Science from <a href="http://www.cornell.edu/">Cornell University</a> and <a href="https://tech.cornell.edu/">Cornell Tech</a> in 2019, advised by Professor <a href="http://blogs.cornell.edu/techfaculty/serge-belongie/">Serge Belongie</a>. Together with the team, I received the <a href="https://tc.computer.org/tcpami/awards/pami-mark-everingham-prize/">PAMI Mark Everingham Prize</a> (2023) for the <a href="https://cocodataset.org/#home">COCO dataset</a>. My research interests lie in Computer Vision and Machine Learning.</p>

    <div id="profile">
      <ul class="social-icon" aria-hidden="true">
        
        
        <li>
          <a href="mailto:richardaecn@gmail.com" target="_blank">
            <i class="fa fa-envelope big-icon"></i>
          </a>
        </li>
        
        
        <li>
          <a href="https://scholar.google.com/citations?user=iP5m52IAAAAJ&amp;hl=en" target="_blank">
            <i class="ai ai-google-scholar big-icon"></i>
          </a>
        </li>
        
        
        <li>
          <a href="https://twitter.com/YinCuiCV" target="_blank">
            <i class="fa fa-twitter big-icon"></i>
          </a>
        </li>
        
        
        <li>
          <a href="https://github.com/richardaecn" target="_blank">
            <i class="fa fa-github big-icon"></i>
          </a>
        </li>
        
        
        <li>
          <a href="https://www.linkedin.com/pub/yin-cui/49/61a/5a8" target="_blank">
            <i class="fa fa-linkedin big-icon"></i>
          </a>
        </li>
        
        
        <li>
          <a href="https://www.facebook.com/yincui1989" target="_blank">
            <i class="fa fa-facebook big-icon"></i>
          </a>
        </li>
        
      </ul>
    </div>
  </div>

</div>

    </div>
  </section>
  

  
  
  
  <section id="lists" class="home-section">
    <div class="container">
      


<div class="row">
  <div class="col-xs-12">
    <h1>Preprints</h1>
    
<ul>

<li><p><strong>Alternating Gradient Descent and Mixture-of-Experts for Integrated Multimodal Perception</strong><br />
Hassan Akbari, Dan Kondratyuk, <u>Yin Cui</u>, Rachel Hornung, Huisheng Wang, Hartwig Adam<br />
<em><strong>NeurIPS</strong> 2023</em><br />
[<a href="https://arxiv.org/abs/2305.06324">arXiv</a>]</p></li>

<li><p><strong>DaTaSeg: Taming a Universal Multi-Dataset Multi-Task Segmentation Model</strong><br />
Xiuye Gu, <u>Yin Cui</u>, Jonathan Huang, Abdullah Rashwan, Xuan Yang, Xingyi Zhou, Golnaz Ghiasi, Weicheng Kuo, Huizhong Chen, Liang-Chieh Chen, David A Ross<br />
<em><strong>NeurIPS</strong> 2023</em><br />
[<a href="https://arxiv.org/abs/2306.01736">arXiv</a>]</p></li>

<li><p><strong>Module-wise Adaptive Distillation for Multimodality Foundation Models</strong><br />
Chen Liang, Jiahui Yu, Ming-Hsuan Yang, Matthew Brown, <u>Yin Cui</u>, Tuo Zhao, Boqing Gong, Tianyi Zhou<br />
<em><strong>NeurIPS</strong> 2023</em><br />
[<a href="https://arxiv.org/abs/2310.04550">arXiv</a>]</p></li>

<li><p><strong>Unified Visual Relationship Detection with Vision and Language Models</strong><br />
Long Zhao, Liangzhe Yuan, Boqing Gong, <u>Yin Cui</u>, Florian Schroff, Ming-Hsuan Yang, Hartwig Adam, Ting Liu<br />
<em><strong>ICCV</strong> 2023</em><br />
[<a href="https://arxiv.org/abs/2303.08998">arXiv</a>]</p></li>

<li><p><strong>A Simple Zero-shot Prompt Weighting Technique to Improve Prompt Ensembling in Text-Image Models</strong><br />
James Urquhart Allingham, Jie Ren, Michael W Dusenberry, Jeremiah Zhe Liu, Xiuye Gu, <u>Yin Cui</u>, Dustin Tran, Balaji Lakshminarayanan<br />
<em><strong>ICML</strong> 2023</em><br />
[<a href="https://arxiv.org/abs/2302.06235">arXiv</a>]</p></li>

<li><p><strong>F-VLM: Open-Vocabulary Object Detection upon Frozen Vision and Language Models</strong><br />
Weicheng Kuo, <u>Yin Cui</u>, Xiuye Gu, AJ Piergiovanni, Anelia Angelova<br />
<em><strong>ICLR</strong> 2023</em><br />
[<a href="https://arxiv.org/abs/2209.15639">arXiv</a>] [<a href="https://sites.google.com/view/f-vlm/home">Website</a>]</p></li>

<li><p><strong>Scaling Open-Vocabulary Image Segmentation with Image-Level Labels</strong><br />
Golnaz Ghiasi, Xiuye Gu, <u>Yin Cui</u>, Tsung-Yi Lin<br />
<em><strong>ECCV</strong> 2022</em><br />
[<a href="https://arxiv.org/abs/2112.12143">arXiv</a>] [<a href="https://github.com/tensorflow/tpu/tree/641c1ac6e26ed788327b973582cbfa297d7d31e7/models/official/detection/projects/openseg">Code</a>]</p></li>

<li><p><strong>Open-vocabulary Object Detection via Vision and Language Knowledge Distillation</strong><br />
Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, <u>Yin Cui</u><br />
<em><strong>ICLR</strong> 2022</em><br />
[<a href="https://arxiv.org/abs/2104.13921">arXiv</a>] [<a href="https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/vild">Code</a>] [<a href="https://colab.research.google.com/github/tensorflow/tpu/blob/master/models/official/detection/projects/vild/ViLD_demo.ipynb">Demo</a>]</p></li>

<li><p><strong>Surrogate Gap Minimization Improves Sharpness-Aware Training</strong><br />
Juntang Zhuang, Boqing Gong, Liangzhe Yuan, <u>Yin Cui</u>, Hartwig Adam, Nicha C. Dvornek, Sekhar Tatikonda, James S. Duncan, Ting Liu<br />
<em><strong>ICLR</strong> 2022</em><br />
[<a href="https://arxiv.org/abs/2203.08065">arXiv</a>] [<a href="https://sites.google.com/view/gsam-iclr22/home">Website</a>] [<a href="https://github.com/juntang-zhuang/GSAM">Code (in PyTorch)</a>] [<a href="https://github.com/google-research/vision_transformer">Models (in JAX)</a>]</p></li>

<li><p><strong>VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text</strong><br />
Hassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, <u>Yin Cui</u>, Boqing Gong<br />
<em><strong>NeurIPS</strong> 2021</em><br />
[<a href="https://arxiv.org/abs/2104.11178">arXiv</a>] [<a href="https://github.com/google-research/google-research/tree/master/vatt">Code</a>]</p></li>

<li><p><strong>Spatiotemporal Contrastive Video Representation Learning</strong><br />
Rui Qian*, Tianjian Meng*, Boqing Gong, Ming-Hsuan Yang, Huisheng Wang, Serge Belongie, <u>Yin Cui</u><br />
<em><strong>CVPR</strong> 2021</em><br />
[<a href="https://arxiv.org/abs/2008.03800">arXiv</a>] [<a href="https://github.com/tensorflow/models/tree/master/official/vision/beta/projects/video_ssl">Code</a>]</p></li>

<li><p><strong>Simple Copy-Paste is a Strong Data Augmentation Method for Instance Segmentation</strong><br />
Golnaz Ghiasi*, <u>Yin Cui</u>*, Aravind Srinivas*, Rui Qian, Tsung-Yi Lin, Ekin D. Cubuk, Quoc V. Le, Barret Zoph<br />
<em><strong>CVPR</strong> 2021</em> (<strong>Oral</strong>)<br />
[<a href="https://arxiv.org/abs/2012.07177">arXiv</a>] [<a href="https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/copy_paste">Code</a>]</p></li>

<li><p><strong>Rethinking Pre-training and Self-training</strong><br />
Barret Zoph*, Golnaz Ghiasi*, Tsung-Yi Lin*, <u>Yin Cui</u>, Hanxiao Liu, Ekin D. Cubuk, Quoc V. Le<br />
<em><strong>NeurIPS</strong> 2020</em> (<strong>Oral</strong>)<br />
[<a href="https://arxiv.org/abs/2006.06882">arXiv</a>] [<a href="https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/self_training">Code</a>]</p></li>

<li><p><strong>Fashionpedia: Ontology, Segmentation, and an Attribute Localization Dataset</strong><br />
Menglin Jia*, Mengyun Shi*, Mikhail Sirotenko*, <u>Yin Cui</u>*, Claire Cardie, Bharath Hariharan, Hartwig Adam, Serge Belongie<br />
<em><strong>ECCV</strong> 2020</em> (<strong>Oral</strong>)<br />
[<a href="https://fashionpedia.github.io/home/index.html">Website</a>] [<a href="https://arxiv.org/abs/2004.12276">arXiv</a>] [<a href="https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/fashionpedia">Code</a>] [<a href="https://www.kaggle.com/c/imaterialist-fashion-2020-fgvc7">Kaggle Challenge</a>]</p></li>

<li><p><strong>SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</strong><br />
Xianzhi Du, Tsung-Yi Lin, Pengchong Jin, Golnaz Ghiasi, Mingxing Tan, <u>Yin Cui</u>, Quoc V. Le, Xiaodan Song<br />
<em><strong>CVPR</strong> 2020</em><br />
[<a href="https://arxiv.org/abs/1912.05027">arXiv</a>] [<a href="https://github.com/tensorflow/tpu/tree/master/models/official/detection">Code</a>] [<a href="https://ai.googleblog.com/2020/06/spinenet-novel-architecture-for-object.html">Google AI Blog</a>]</p></li>

<li><p><strong>Class-Balanced Loss Based on Effective Number of Samples</strong><br />
<u>Yin Cui</u>, Menglin Jia, Tsung-Yi Lin, Yang Song, Serge Belongie<br />
<em><strong>CVPR</strong> 2019</em><br />
[<a href="https://arxiv.org/abs/1901.05555">arXiv</a>] [<a href="https://github.com/richardaecn/class-balanced-loss">Code</a>] [<a href="posters/CVPR19_Class-Balanced.pdf">Poster</a>]</p></li>

<li><p><strong>Large Scale Fine-Grained Categorization and Domain-Specific Transfer Learning</strong><br />
<u>Yin Cui</u>, Yang Song, Chen Sun, Andrew Howard, Serge Belongie<br />
<em><strong>CVPR</strong> 2018</em><br />
[<a href="https://arxiv.org/abs/1806.06193">arXiv</a>] [<a href="https://github.com/visipedia/inat_comp/tree/master/2017">Data</a>] [<a href="https://github.com/richardaecn/cvpr18-inaturalist-transfer">Code</a>] [<a href="posters/CVPR18_FGVC.pdf">Poster</a>] [<a href="https://tfhub.dev/google/inaturalist/inception_v3/feature_vector/4">Tensorflow Hub</a>]</p></li>

<li><p><strong>The iNaturalist Species Classification and Detection Dataset</strong><br />
Grant Van Horn, Oisin Mac Aodha, Yang Song, <u>Yin Cui</u>, Chen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, Serge Belongie<br />
<em><strong>CVPR</strong> 2018</em> (<strong>Spotlight</strong>)<br />
[<a href="https://arxiv.org/abs/1707.06642">arXiv</a>] [<a href="https://github.com/visipedia/inat_comp">Data</a>] [<a href="https://github.com/tensorflow/models/tree/master/research/object_detection#sep-17-2018">Tensorflow Object Detection API</a>] [<a href="https://ai.googleblog.com/2018/03/introducing-inaturalist-2018-challenge.html">Google AI Blog</a>] [<a href="https://techcrunch.com/2018/06/21/species-identifying-ai-gets-a-boost-from-images-snapped-by-citizen-naturalists/">TechCrunch</a>]</p></li>

<li><p><strong>Collaborative Metric Learning</strong><br />
Cheng-Kang Hsieh, Longqi Yang, <u>Yin Cui</u>, Tsung-Yi Lin, Serge Belongie, Deborah Estrin<br />
<em><strong>WWW</strong> 2017</em><br />
[<a href="papers/WWW17_CML.pdf">PDF</a>] [<a href="https://github.com/changun/CollMetric">Code</a>] [<a href="slides/WWW17_CML.pdf">Slides</a>]</p></li>

<li><p><strong>Learning Deep Representations for Ground-to-Aerial Geolocalization</strong><br />
Tsung-Yi Lin, <u>Yin Cui</u>, Serge Belongie, James Hays<br />
<em><strong>CVPR</strong> 2015</em> (<strong>Oral</strong>)<br />
[<a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Lin_Learning_Deep_Representations_2015_CVPR_paper.pdf">PDF</a>] [<a href="https://drive.google.com/folderview?id=0B6Udwolfp4WYUkhRYTNneUhXWEU&usp=sharing">Data</a>] [<a href="papers/CVPR15_Geolocalization_Abstract.pdf">Extended Abstract</a>] [<a href="posters/CVPR15_DeepGeo.pdf">Poster</a>]</p></li>
</ul>

  </div>
</div>

    </div>
  </section>
  

  
  
  
  <section id="misc" class="home-section">
    <div class="container">
      


<div class="row">
  <div class="col-xs-12">
    <h1>Miscellaneous</h1>
    
    

<h3 id="professional-activities">Professional Activities</h3>

<ul>
<li>Area Chair of ICLR 2024, NeurIPS 2023, ICCV 2023, WACV 2023</li>
<li>Senior Program Committee (SPC) Member of AAAI 2022, AAAI 2023</li>
<li>Guest Editor of IJCV Special Issue on Open-World Visual Recognition</li>
<li>Reviewer of TPAMI, IJCV, CVPR, ICCV, ECCV, NeurIPS, ICML</li>
<li>Organizing Committee of ImageNet and COCO Visual Recognition Workshop at ICCV 2015, ECCV 2016</li>
<li>Organizing Committee of Joint Workshop of the COCO and Places Challenges at ICCV 2017</li>
<li>Organizing Committee of Joint COCO and Mapilary Recognition Challenge Workshop at ECCV 2018, ICCV 2019</li>
<li>Organizing Committee of Joint COCO and LVIS Recognition Challenge Workshop at ECCV 2020</li>
<li>Organizing Committee of Fine-Grained Visual Categorization Workshop at CVPR 2017, CVPR 2018, CVPR 2019</li>
<li>Organizing Committee of Large-scale Scene Understanding Workshop (COCO Captioning Challenge) at CVPR 2015</li>
</ul>

<h3 id="selected-honors">Selected Honors</h3>

<ul>
<li>PAMI Mark Everingham Prize (2023)</li>  
<li>McMullen Fellowship (2014 - 2015)</li>
<li>Edwin Howard Armstrong Memorial Award (2014)</li>
<li>Wei Family Private Foundation Special Scholarship (2013)</li>
<li>National Scholarship (2010)</li>
</ul>

  </div>
</div>

    </div>
  </section>
  



<footer class="site-footer">
  <div class="container">
    <p class="powered-by">

      &copy; 2023 Yin Cui &middot; 

      Powered by the
      <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
      <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close btn-large" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">Cite</h4>
      </div>
      <div>
        <pre><code class="modal-body tex"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-primary btn-outline js-copy-cite" href="#" target="_blank">
          <i class="fa fa-copy"></i> Copy
        </a>
        <a class="btn btn-primary btn-outline js-download-cite" href="#" target="_blank">
          <i class="fa fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    
    <script id="dsq-count-scr" src="//richardaecn.disqus.com/count.js" async></script>
    

    

    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js" integrity="sha512-3P8rXCuGJdNZOnUx/03c1jOTnMn3rP63nBip5gOP2qmUh5YAdVAvFZ1E+QLZZbC1rtMrQb+mah3AfYW11RUrWA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>
    
    
    <script src="/js/hugo-academic.js"></script>
    

    
    
      
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
      

      

      

      <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
    </script>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML" integrity="sha512-tOav5w1OjvsSJzePRtt2uQPFwBoHt1VZcUq8l8nm5284LEKE9FSJBQryzMBzHxY5P0zRdNqEcpLIRVYFNgu1jw==" crossorigin="anonymous"></script>
    
    

  </body>
</html>
