<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.40.1" />
  <meta name="author" content="Yin Cui">

  
  
  
  
  <meta name="description" content="Research Scientist">

  
  <link rel="alternate" hreflang="en-us" href="/">

  


  

  
  
  <meta name="theme-color" content="#0095eb">
  
  
  
  
    
  
  
    
    
      
        <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
      
    
  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.1/css/academicons.min.css" integrity="sha512-NThgw3XKQ1absAahW6to7Ey42uycrVvfNfyjqcFNgCmOCQ5AR4AO0SiXrN+8ZtYeappp56lk1WtvjVmEa+VR6A==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">
  
  
  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700%7cRoboto:400,400italic,700%7cRoboto&#43;Mono">
  
  <link rel="stylesheet" href="/styles.css">
  
  <link rel="stylesheet" href="/css/yincui.css">
  

  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-54472398-1', 'auto');
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="//www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  

  
  <link rel="alternate" href="/index.xml" type="application/rss+xml" title="Yin Cui">
  <link rel="feed" href="/index.xml" type="application/rss+xml" title="Yin Cui">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@https://twitter.com/YinCui1">
  <meta property="twitter:creator" content="@https://twitter.com/YinCui1">
  
  <meta property="og:site_name" content="Yin Cui">
  <meta property="og:url" content="/">
  <meta property="og:title" content="Yin Cui">
  <meta property="og:description" content="Research Scientist">
  <meta property="og:locale" content="en-us">
  
  <meta property="og:updated_time" content="2023-03-07T00:00:00&#43;00:00">
  

  

  <title>Yin Cui</title>

</head>
<body id="top" data-spy="scroll" data-target="#navbar-main" data-offset="71" >

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
      <a class="navbar-brand" href="/">Yin Cui</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      
      <ul class="nav navbar-nav navbar-right">
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#about" data-target="#about">
            
            <span>About</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#lists" data-target="#lists">
            
            <span>Research</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#misc" data-target="#misc">
            
            <span>Misc</span>
            
          </a>
        </li>

        
        
      

      
      </ul>

    </div>
  </div>
</nav>



<span id="homepage" style="display: none"></span>


  





  
  
  
  <section id="about" class="home-section">
    <div class="container">
      



<div class="row">
  <div class="col-lg-12 col-md-4">
    <div id="profile">

      
      <div class="portrait" itemprop="image"
           style="background-image: url('/img/portrait.jpg');">
      </div>
      

      <div class="portrait-title">
        <h2 itemprop="name">Yin Cui</h2>
        <h3 itemprop="jobTitle">Research Scientist at NVIDIA</h3>  
      </div>
    </div>
  </div>

  <div class="col-lg-12 col-md-8" itemprop="description">
    <h1>About</h1>
    
    <p>I am Yin Cui (崔崟 in Chinese, pronounced as /yin tsui/), a research scientist at NVIDIA. Before joining NVIDIA, I was a research scientist at Google. I obtained my Ph.D. in Computer Science from <a href="http://www.cornell.edu/">Cornell University</a> and <a href="https://tech.cornell.edu/">Cornell Tech</a> in 2019, advised by Professor <a href="http://blogs.cornell.edu/techfaculty/serge-belongie/">Serge Belongie</a>. Together with the team, I received the <a href="https://tc.computer.org/tcpami/awards/pami-mark-everingham-prize/">PAMI Mark Everingham Prize</a> (2023) for the <a href="https://cocodataset.org/#home">COCO dataset</a>. My current research interests are Generative AI and Multimodal.</p>

    <div id="profile">
      <ul class="social-icon" aria-hidden="true">
        
        <li>
          <a href="mailto:richardaecn@gmail.com" target="_blank">
            <i class="fa fa-envelope big-icon"></i>
          </a>
        </li>
        
        <li>
          <a href="https://scholar.google.com/citations?user=iP5m52IAAAAJ&amp;hl=en" target="_blank">
            <i class="ai ai-google-scholar big-icon"></i>
          </a>
        </li>
        
        <li>
          <a href="https://twitter.com/YinCuiCV" target="_blank">
            <i class="fa fa-twitter big-icon"></i>
          </a>
        </li>
        
        <li>
          <a href="https://www.linkedin.com/pub/yin-cui/49/61a/5a8" target="_blank">
            <i class="fa fa-linkedin big-icon"></i>
          </a>
        </li>
        
      </ul>
    </div>
  </div>

</div>

    </div>
  </section>
  

  
  
  
<section id="lists" class="home-section">
<div class="container">  

<div class="row">
  <div class="col-lg-12">
    <h1>Industry Research</h1>

<ul>

<div class="publication-entry">
  <div class="teaser-video col-lg-3">
    <video width="300" autoplay loop muted>
      <source src="img/paper/Cosmos1.mp4" type="video/mp4">
    </video>
  </div>
  <div class="publication-text col-lg-9">
    <p>
      <strong>Cosmos World Foundation Model Platform for Physical AI</strong><br />
      NVIDIA: <u>Yin Cui</u> (core contributor)<br />
      [<a href="https://arxiv.org/abs/2501.03575">Paper</a>] [<a href="https://www.nvidia.com/en-us/ai/cosmos/">Website</a>] [<a href="https://github.com/NVIDIA/Cosmos">Code</a>] [<a href="https://huggingface.co/collections/nvidia/cosmos-6751e884dc10e013a0a0d8e6">Hugging Face</a>]<br />
      [<a href="https://research.nvidia.com/labs/dir/cosmos1/">Project Page</a>] [<a href="https://blogs.nvidia.com/blog/cosmos-world-foundation-models/">Blog</a>] [<a href="https://www.youtube.com/watch?v=9Uch931cDx8">Video</a>] [<a href="https://build.nvidia.com/nvidia/cosmos-1_0-diffusion-7b">Model API</a>]<br />
      [<a href="https://www.youtube.com/live/k82RwXqZHY8?t=3303s">Jensen Huang Keynote at CES 2025</a>]
    </p>
  </div>
</div>

<div class="publication-entry">
  <div class="teaser-video col-lg-3">
    <video width="300" autoplay loop muted>
      <source src="img/paper/Edify-3D.mp4" type="video/mp4">
    </video>
  </div>
  <div class="publication-text col-lg-9">
    <p>
      <strong>Edify 3D: Scalable High-Quality 3D Asset Generation</strong><br />
      NVIDIA: <u>Yin Cui</u> (core contributor)<br />
      [<a href="https://arxiv.org/abs/2411.07135">Paper</a>] [<a href="https://research.nvidia.com/labs/dir/edify-3d/">Website</a>] [<a href="https://www.youtube.com/watch?v=ROqB8xhKZ6U">Video</a>] [<a href="https://build.nvidia.com/shutterstock/edify-3d">Model API</a>]
    </p>
  </div>
</div>

<div class="publication-entry">
  <div class="teaser-image col-lg-3">
    <img src="img/paper/Edify-Image.gif">
  </div>
  <div class="publication-text col-lg-9">
    <p>
      <strong>Edify Image: High-Quality Image Generation with Pixel Space Laplacian Diffusion Models</strong><br />
      NVIDIA: <u>Yin Cui</u> (core contributor)<br />
      [<a href="https://arxiv.org/abs/2411.07126">Paper</a>] [<a href="https://research.nvidia.com/labs/dir/edify-image/">Website</a>] [<a href="https://www.youtube.com/watch?v=opiaV94tMJ4">Video</a>] [<a href="https://build.nvidia.com/gettyimages/edify-image">Image Model API</a>] [<a href="https://build.nvidia.com/shutterstock/edify-360-hdri">360 HDRi Model API</a>]
    </p>
  </div>
</div>

<div class="publication-entry">
  <div class="teaser-image col-lg-3">
    <img src="img/paper/SIGGRAPHRTL24_GenUSD.jpg">
  </div>
  <div class="publication-text col-lg-9">
    <p>
      <strong>GenUSD: 3D Scene Generation Made Easy</strong><br />
      Jiashu Xu, Yunhao Ge, Yifan Ding, <u>Yin Cui</u>, Chen-Hsuan Lin, Xiaohui Zeng, Zekun Hao, Zhaoshuo Li, Donglai Xiang, Qianli Ma, Fangyin Wei, JP Lewis, Qinsheng Zhang, Seungjun Nah, Arun Mallya, Jingyi Jin, Hanzi Mao, Yen-Chen Lin, Pooya Jannaty, Tsung-Yi Lin, Ming-Yu Liu<br />
      <em><strong>ACM SIGGRAPH Real-Time Live!</strong> 2024</em><br />
      [<a href="https://www.youtube.com/watch?v=Gm1B5DT8kE0&t=1752s">Live Demo at SIGGRAPH 2024</a>] [<a href="https://blogs.nvidia.com/blog/real-time-3d-generative-ai-research-siggraph-2024/">Blog</a>] [<a href="https://dl.acm.org/doi/pdf/10.1145/3641520.3665306">Paper</a>] [<a href="https://youtu.be/AJWTUvXA0Wc?si=XVzdqKR7zy8fSN4k">Video</a>]
    </p>
  </div>
</div>

</div>
</div>
</section>




<section id="lists" class="home-section">
<div class="container">

<div class="row">
  <div class="col-lg-12">
    <h1>Selected Publications</h1>

<ul>

<div class="publication-entry">
  <div class="teaser-image col-lg-3">
    <img src="img/paper/TMLR24_VideoGLUE.png">
  </div>
  <div class="publication-text col-lg-9">
    <p>
      <strong>VideoGLUE: Video General Understanding Evaluation of Foundation Models</strong><br />
      Liangzhe Yuan, Nitesh Bharadwaj Gundavarapu, Long Zhao, Hao Zhou, <u>Yin Cui</u>, Lu Jiang, Xuan Yang, Menglin Jia, Tobias Weyand, Luke Friedman, Mikhail Sirotenko, Huisheng Wang, Florian Schroff, Hartwig Adam, Ming-Hsuan Yang, Ting Liu, Boqing Gong<br />
      <em><strong>TMLR</strong> 2024</em><br />
      [<a href="https://arxiv.org/abs/2307.03166">Paper</a>] [<a href="https://github.com/tensorflow/models/tree/master/official/projects/videoglue">Code</a>]
    </p>
  </div>
</div>

<div class="publication-entry">
  <div class="teaser-image col-lg-3">
    <img src="img/paper/TMLR24_FG.png">
  </div>
  <div class="publication-text col-lg-9">
    <p>
      <strong>Why Fine-grained Labels in Pretraining Benefit Generalization?</strong><br />
      Guan Zhe Hong, <u>Yin Cui</u>, Ariel Fuxman, Stanley Chan, Enming Luo<br />
      <em><strong>TMLR</strong> 2024</em><br />
      [<a href="https://arxiv.org/abs/2410.23129">Paper</a>]
    </p>
  </div>
</div>

<div class="publication-entry">
  <div class="teaser-image col-lg-3">
    <img src="img/paper/CVPR24_VFC.png">
  </div>
  <div class="publication-text col-lg-9">
    <p>
      <strong>Visual Fact Checker: Enabling High-Fidelity Detailed Caption Generation</strong><br />
      Yunhao Ge, Xiaohui Zeng, Jacob Samuel Huffman, Tsung-Yi Lin, Ming-Yu Liu, <u>Yin Cui</u><br />
      <em><strong>CVPR</strong> 2024</em><br />
      [<a href="https://arxiv.org/abs/2404.19752">Paper</a>] [<a href="https://research.nvidia.com/labs/dir/vfc/">Website</a>]
    </p>
  </div>
</div>

<div class="publication-entry">
  <div class="teaser-image col-lg-3">
    <img src="img/paper/NeurIPS23_IMP.png">
  </div>
  <div class="publication-text col-lg-9">
    <p>
      <strong>Alternating Gradient Descent and Mixture-of-Experts for Integrated Multimodal Perception</strong><br />
      Hassan Akbari, Dan Kondratyuk, <u>Yin Cui</u>, Rachel Hornung, Huisheng Wang, Hartwig Adam<br />
      <em><strong>NeurIPS</strong> 2023</em><br />
      [<a href="https://arxiv.org/abs/2305.06324">Paper</a>] [<a href="https://github.com/google-research/google-research/tree/master/imp">Code</a>]
    </p>
  </div>
</div>

<div class="publication-entry">
  <div class="teaser-image col-lg-3">
    <img src="img/paper/NeurIPS23_DataSeg.png">
  </div>
  <div class="publication-text col-lg-9">
    <p>
      <strong>DaTaSeg: Taming a Universal Multi-Dataset Multi-Task Segmentation Model</strong><br />
      Xiuye Gu, <u>Yin Cui</u>, Jonathan Huang, Abdullah Rashwan, Xuan Yang, Xingyi Zhou, Golnaz Ghiasi, Weicheng Kuo, Huizhong Chen, Liang-Chieh Chen, David A Ross<br />
      <em><strong>NeurIPS</strong> 2023</em><br />
      [<a href="https://arxiv.org/abs/2306.01736">Paper</a>] [<a href="https://github.com/google-research-datasets/DaTaSeg-Objects365-Instance-Segmentation">Objects365 Instance Segmentation Dataset</a>]
    </p>
  </div>
</div>

<div class="publication-entry">
  <div class="teaser-image col-lg-3">
    <img src="img/paper/ICML23_ZPE.png">
  </div>
  <div class="publication-text col-lg-9">
    <p>
      <strong>A Simple Zero-shot Prompt Weighting Technique to Improve Prompt Ensembling in Text-Image Models</strong><br />
      James Urquhart Allingham, Jie Ren, Michael W Dusenberry, Jeremiah Zhe Liu, Xiuye Gu, <u>Yin Cui</u>, Dustin Tran, Balaji Lakshminarayanan<br />
      <em><strong>ICML</strong> 2023</em><br />
      [<a href="https://arxiv.org/abs/2302.06235">Paper</a>] [<a href="https://github.com/google/uncertainty-baselines/tree/main/experimental/multimodal">Code</a>]
    </p>
  </div>
</div>

<div class="publication-entry">
  <div class="teaser-image col-lg-3">
    <img src="img/paper/ICLR23_FVLM.png">
  </div>
  <div class="publication-text col-lg-9">
    <p>
      <strong>F-VLM: Open-Vocabulary Object Detection upon Frozen Vision and Language Models</strong><br />
      Weicheng Kuo, <u>Yin Cui</u>, Xiuye Gu, AJ Piergiovanni, Anelia Angelova<br />
      <em><strong>ICLR</strong> 2023</em><br />
      [<a href="https://arxiv.org/abs/2209.15639">Paper</a>] [<a href="https://sites.google.com/view/f-vlm">Website</a>] [<a href="https://github.com/google-research/google-research/tree/master/fvlm">Code</a>]
    </p>
  </div>
</div>

<div class="publication-entry">
  <div class="teaser-image col-lg-3">
    <img src="img/paper/ECCV22_OpenSeg.png">
  </div>
  <div class="publication-text col-lg-9">
    <p>
      <strong>Scaling Open-Vocabulary Image Segmentation with Image-Level Labels</strong><br />
      Golnaz Ghiasi, Xiuye Gu, <u>Yin Cui</u>, Tsung-Yi Lin<br />
      <em><strong>ECCV</strong> 2022</em><br />
      [<a href="https://arxiv.org/abs/2112.12143">Paper</a>] [<a href="https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/openseg">Code</a>]
    </p>
  </div>
</div>

<div class="publication-entry">
  <div class="teaser-image col-lg-3">
    <img src="img/paper/ICLR22_ViLD.png">
  </div>
  <div class="publication-text col-lg-9">
    <p>
      <strong>Open-vocabulary Object Detection via Vision and Language Knowledge Distillation</strong><br />
      Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, <u>Yin Cui</u><br />
      <em><strong>ICLR</strong> 2022</em><br />
      [<a href="https://arxiv.org/abs/2104.13921">Paper</a>] [<a href="https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/vild">Code</a>]
    </p>
  </div>
</div>

<div class="publication-entry">
  <div class="teaser-image col-lg-3">
    <img src="img/paper/ICLR22_GSAM.png">
  </div>
  <div class="publication-text col-lg-9">
    <p>
      <strong>Surrogate Gap Minimization Improves Sharpness-Aware Training</strong><br />
      Juntang Zhuang, Boqing Gong, Liangzhe Yuan, <u>Yin Cui</u>, Hartwig Adam, Nicha C. Dvornek, Sekhar Tatikonda, James S. Duncan, Ting Liu<br />
      <em><strong>ICLR</strong> 2022</em><br />
      [<a href="https://arxiv.org/abs/2203.08065">Paper</a>] [<a href="https://sites.google.com/view/gsam-iclr22/home">Website</a>] [<a href="https://github.com/juntang-zhuang/GSAM">Code in PyTorch</a>] [<a href="https://github.com/google-research/big_vision/blob/main/big_vision/trainers/proj/gsam/gsam.py">Code in JAX</a>]
    </p>
  </div>
</div>

<div class="publication-entry">
  <div class="teaser-image col-lg-3">
    <img src="img/paper/NeurIPS21_VATT.png">
  </div>
  <div class="publication-text col-lg-9">
    <p>
      <strong>VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text</strong><br />
      Hassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, <u>Yin Cui</u>, Boqing Gong<br />
      <em><strong>NeurIPS</strong> 2021</em><br />
      [<a href="https://arxiv.org/abs/2104.11178">Paper</a>] [<a href="https://github.com/google-research/google-research/tree/master/vatt">Code</a>]
    </p>
  </div>
</div>

<div class="publication-entry">
  <div class="teaser-image col-lg-3">
    <img src="img/paper/CVPR21_CVRL.png">
  </div>
  <div class="publication-text col-lg-9">
    <p>
      <strong>Spatiotemporal Contrastive Video Representation Learning</strong><br />
      Rui Qian*, Tianjian Meng*, Boqing Gong, Ming-Hsuan Yang, Huisheng Wang, Serge Belongie, <u>Yin Cui</u><br />
      <em><strong>CVPR</strong> 2021</em><br />
      [<a href="https://arxiv.org/abs/2008.03800">Paper</a>] [<a href="https://github.com/tensorflow/models/tree/master/official/projects/video_ssl">Code</a>]
    </p>
  </div>
</div>

<div class="publication-entry">
  <div class="teaser-image col-lg-3">
    <img src="img/paper/CVPR21_CopyPaste.png">
  </div>
  <div class="publication-text col-lg-9">
    <p>
      <strong>Simple Copy-Paste is a Strong Data Augmentation Method for Instance Segmentation</strong><br />
      Golnaz Ghiasi*, <u>Yin Cui</u>*, Aravind Srinivas*, Rui Qian, Tsung-Yi Lin, Ekin D. Cubuk, Quoc V. Le, Barret Zoph<br />
      <em><strong>CVPR</strong> 2021</em> (<strong>Oral</strong>)<br />
      [<a href="https://arxiv.org/abs/2012.07177">Paper</a>] [<a href="https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/copy_paste">Code</a>]
    </p>
  </div>
</div>

<div class="publication-entry">
  <div class="teaser-image col-lg-3">
    <img src="img/paper/NeurIPS20_Selftraining.png">
  </div>
  <div class="publication-text col-lg-9">
    <p>
      <strong>Rethinking Pre-training and Self-training</strong><br />
      Barret Zoph*, Golnaz Ghiasi*, Tsung-Yi Lin*, <u>Yin Cui</u>, Hanxiao Liu, Ekin D. Cubuk, Quoc V. Le<br />
      <em><strong>NeurIPS</strong> 2020</em> (<strong>Oral</strong>)<br />
      [<a href="https://arxiv.org/abs/2006.06882">Paper</a>] [<a href="https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/self_training">Code</a>]
    </p>
  </div>
</div>

<div class="publication-entry">
  <div class="teaser-image col-lg-3">
    <img src="img/paper/ECCV20_Fashionpedia.jpg">
  </div>
  <div class="publication-text col-lg-9">
    <p>
      <strong>Fashionpedia: Ontology, Segmentation, and an Attribute Localization Dataset</strong><br />
      Menglin Jia*, Mengyun Shi*, Mikhail Sirotenko*, <u>Yin Cui</u>*, Claire Cardie, Bharath Hariharan, Hartwig Adam, Serge Belongie<br />
      <em><strong>ECCV</strong> 2020</em> (<strong>Oral</strong>)<br />
      [<a href="https://arxiv.org/abs/2004.12276">Paper</a>] [<a href="https://fashionpedia.github.io/home/index.html">Website</a>] [<a href="https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/fashionpedia">Code</a>] [<a href="https://www.kaggle.com/c/imaterialist-fashion-2020-fgvc7">Kaggle Challenge</a>]
    </p>
  </div>
</div>

<div class="publication-entry">
  <div class="teaser-image col-lg-3">
    <img src="img/paper/CVPR19_ClassBalancedLoss.png">
  </div>
  <div class="publication-text col-lg-9">
    <p>
      <strong>Class-Balanced Loss Based on Effective Number of Samples</strong><br />
      <u>Yin Cui</u>, Menglin Jia, Tsung-Yi Lin, Yang Song, Serge Belongie<br />
      <em><strong>CVPR</strong> 2019</em><br />
      [<a href="https://arxiv.org/abs/1901.05555">Paper</a>] [<a href="https://github.com/richardaecn/class-balanced-loss">Code</a>]
    </p>
  </div>
</div>

<div class="publication-entry">
  <div class="teaser-image col-lg-3">
    <img src="img/paper/CVPR18_FGVC.png">
  </div>
  <div class="publication-text col-lg-9">
    <p>
      <strong>Large Scale Fine-Grained Categorization and Domain-Specific Transfer Learning</strong><br />
      <u>Yin Cui</u>, Yang Song, Chen Sun, Andrew Howard, Serge Belongie<br />
      <em><strong>CVPR</strong> 2018</em><br />
      [<a href="https://arxiv.org/abs/1806.06193">Paper</a>] [<a href="https://github.com/richardaecn/cvpr18-inaturalist-transfer">Code</a>] [<a href="https://github.com/visipedia/inat_comp/tree/master/2017">Data</a>] [<a href="https://www.kaggle.com/models/google/inception-v3/tensorFlow2/inaturalist-inception-v3-feature-vector">Tensorflow Hub</a>]
    </p>
  </div>
</div>

<div class="publication-entry">
  <div class="teaser-image col-lg-3">
    <img src="img/paper/CVPR18_iNat.jpg">
  </div>
  <div class="publication-text col-lg-9">
    <p>
      <strong>The iNaturalist Species Classification and Detection Dataset</strong><br />
      Grant Van Horn, Oisin Mac Aodha, Yang Song, <u>Yin Cui</u>, Chen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, Serge Belongie<br />
      <em><strong>CVPR</strong> 2018</em> (<strong>Spotlight</strong>)<br />
      [<a href="https://arxiv.org/abs/1707.06642">Paper</a>] [<a href="https://github.com/visipedia/inat_comp">Code and Data</a>] 
      [<a href="https://github.com/tensorflow/models/tree/master/research/object_detection#sep-17-2018">Tensorflow Object Detection API</a>] [<a href="https://ai.googleblog.com/2018/03/introducing-inaturalist-2018-challenge.html">Blog</a>]
    </p>
  </div>
</div>

<div class="publication-entry">
  <div class="teaser-image col-lg-3">
    <img src="img/paper/WWW17_CML.png">
  </div>
  <div class="publication-text col-lg-9">
    <p>
      <strong>Collaborative Metric Learning</strong><br />
      Cheng-Kang Hsieh, Longqi Yang, <u>Yin Cui</u>, Tsung-Yi Lin, Serge Belongie, Deborah Estrin<br />
      <em><strong>WWW</strong> 2017</em><br />
      [<a href="papers/WWW17_CML.pdf">Paper</a>] [<a href="https://github.com/changun/CollMetric">Code</a>]
    </p>
  </div>
</div>

<div class="publication-entry">
  <div class="teaser-image col-lg-3">
    <img src="img/paper/CVPR15_Geolocalization.png">
  </div>
  <div class="publication-text col-lg-9">
    <p>
      <strong>Learning Deep Representations for Ground-to-Aerial Geolocalization</strong><br />
      Tsung-Yi Lin, <u>Yin Cui</u>, Serge Belongie, James Hays<br />
      <em><strong>CVPR</strong> 2015</em> (<strong>Oral</strong>)<br />
      [<a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Lin_Learning_Deep_Representations_2015_CVPR_paper.pdf">Paper</a>] [<a href="https://drive.google.com/folderview?id=0B6Udwolfp4WYUkhRYTNneUhXWEU&usp=sharing">Data</a>]
    </p>
  </div>
</div>

</ul>

</div>
</div>
</div>
</section>

  
  

<section id="misc" class="home-section">
<div class="container">
      

<div class="row">
<div class="col-lg-12">
<h1>Miscellaneous</h1>

<h3 id="professional-activities">Professional Activities</h3>

<ul>
<li>Area Chair of ICCV 2025, ICLR 2025, NeurIPS 2024, ICLR 2024, NeurIPS 2023, ICCV 2023, WACV 2023</li>
<li>Action Editor of TMLR</li>
<li>Senior Program Committee (SPC) Member of AAAI 2022, AAAI 2023</li>
<li>Guest Editor of IJCV Special Issue on Open-World Visual Recognition</li>
<li>Reviewer of CVPR, ICCV, ECCV, NeurIPS, ICML, ICLR, TPAMI, IJCV</li>
<li>Organizing Committee of ImageNet and COCO Visual Recognition Workshop at ICCV 2015, ECCV 2016</li>
<li>Organizing Committee of Joint Workshop of the COCO and Places Challenges at ICCV 2017</li>
<li>Organizing Committee of Joint COCO and Mapilary Recognition Challenge Workshop at ECCV 2018, ICCV 2019</li>
<li>Organizing Committee of Joint COCO and LVIS Recognition Challenge Workshop at ECCV 2020</li>
<li>Organizing Committee of Fine-Grained Visual Categorization Workshop at CVPR 2017, CVPR 2018, CVPR 2019</li>
<li>Organizing Committee of Large-scale Scene Understanding Workshop (COCO Captioning Challenge) at CVPR 2015</li>
</ul>

<h3 id="selected-honors">Selected Honors</h3>

<ul>
<li>PAMI Mark Everingham Prize (2023)</li>  
<li>McMullen Fellowship (2014 - 2015)</li>
<li>Edwin Howard Armstrong Memorial Award (2014)</li>
<li>Wei Family Private Foundation Special Scholarship (2013)</li>
<li>National Scholarship (2010)</li>
</ul>

  </div>
</div>

    </div>
  </section>
  



<footer class="site-footer">
  <div class="container">
    <p class="powered-by">

      &copy; 2025 Yin Cui &middot; 

      Powered by the
      <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
      <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close btn-large" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">Cite</h4>
      </div>
      <div>
        <pre><code class="modal-body tex"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-primary btn-outline js-copy-cite" href="#" target="_blank">
          <i class="fa fa-copy"></i> Copy
        </a>
        <a class="btn btn-primary btn-outline js-download-cite" href="#" target="_blank">
          <i class="fa fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    
    <script id="dsq-count-scr" src="//richardaecn.disqus.com/count.js" async></script>
    

    

    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js" integrity="sha512-3P8rXCuGJdNZOnUx/03c1jOTnMn3rP63nBip5gOP2qmUh5YAdVAvFZ1E+QLZZbC1rtMrQb+mah3AfYW11RUrWA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>
    
    
    <script src="/js/hugo-academic.js"></script>
    

    
    
      
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
      

      

      

      <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
    </script>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML" integrity="sha512-tOav5w1OjvsSJzePRtt2uQPFwBoHt1VZcUq8l8nm5284LEKE9FSJBQryzMBzHxY5P0zRdNqEcpLIRVYFNgu1jw==" crossorigin="anonymous"></script>
    
    

  </body>
</html>
